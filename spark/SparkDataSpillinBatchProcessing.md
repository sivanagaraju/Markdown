Great! I will conduct a deep dive into Spark data spills in batch processing (Spark Core). This will include:

- Understanding why data spills happen, covering execution memory, storage memory, and their interaction.
- How memory allocation works internally in Spark and its impact on spills.
- Relationship between executor memory and driver memory in the context of data spilling.
- Examples in **Python** that demonstrate spilling in both **standalone mode** and **cluster mode**.
- Tuning memory configurations such as `spark.memory.fraction`, `spark.memory.storageFraction`, and other related settings to mitigate spills.

I will also provide specific memory configurations and real-world examples that show how tuning affects spilling behavior.

I'll get back to you with the detailed findings soon.

# Spark Data Spills in Batch Processing (Spark Core)

When Spark cannot hold intermediate data in memory during computations, it **“spills” data to disk** as a safety net to avoid out-of-memory errors ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=Simply%20put%2C%20this%20behavior%20occurs,for%20the%20remaining%20tasks%20within)) ([Understanding Data Spilling in Databricks and How to Prevent It | by Rui Carvalho | Data Engineer Things](https://blog.det.life/understanding-data-spilling-in-databricks-and-how-to-prevent-it-6b7d2ed36556#:~:text=Data%20spilling%20happens%20when%20Apache,memory%20while%20running%20a%20task)). Spilling is expensive because disk I/O is much slower than RAM, so understanding why spills occur and how Spark manages memory is crucial for optimizing performance. Below we dive into the causes of spills, Spark’s memory management internals, the role of executor vs driver memory, and practical examples and tuning strategies (with PySpark code) to reduce spills.

## Causes of Data Spills to Disk

**Data spills to disk occur when a task’s intermediate data cannot fit in the allotted memory.** In Spark’s batch processing (Spark Core with RDDs), operations like shuffles, sorts, joins, and aggregations may produce large amounts of intermediate data. If a single task (handling one partition of data) generates more data than can be kept in memory, Spark will write the excess to disk ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=Simply%20put%2C%20this%20behavior%20occurs,for%20the%20remaining%20tasks%20within)). For example, a wide operation that groups or sorts a very large partition (or many keys) can exceed memory, triggering a spill. Spilling typically happens per task during shuffle write or aggregation when in-memory buffers or data structures fill up.

Spark chooses to spill rather than crash – it’s essentially Spark’s *last resort* to avoid an out-of-memory (OOM) error ([Understanding common Performance Issues in Apache Spark](https://michaelheil.medium.com/understanding-common-performance-issues-in-apache-spark-deep-dive-data-spill-7cdba81e697e#:~:text=Spark%20michaelheil,to%20avoid%20an%20OutOfMemory)). This ensures the job can complete, but at the cost of performance. **Disk spills significantly slow down processing** because data must be written to and read from disk instead of accessed in RAM ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=Simply%20put%2C%20this%20behavior%20occurs,remaining%20tasks%20within%20the%20job)). Common causes of spilling include: 

- **Large shuffle partitions:** If partitions are too large (e.g., due to insufficient number of partitions or data skew), an executor may not have enough RAM for the partition’s data ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=Spill%20problem%20happens%20when%20the,then%20back%20to%20RAM%20again)). Spark will spill the partition’s overflow to disk.  
- **Heavy aggregations or joins:** Operations like `reduceByKey`, `groupByKey`, and sort-merge joins use in-memory data structures (e.g., hash maps or sort buffers). When these grow beyond the memory available for execution, Spark spills them in stages (e.g., external merge-sort or external aggregation) ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=Simply%20put%2C%20this%20behavior%20occurs,for%20the%20remaining%20tasks%20within)).  
- **Insufficient execution memory allocation:** If Spark’s configuration reserves too little memory for execution (due to memory being used by storage or overhead), even moderate data sizes can trigger spills. We discuss Spark’s memory divisions below – essentially, if the *execution memory* pool is small, spills become more frequent ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=From%20the%20Spark%20configuration%20docs%2C,configuration%20parameter)).  
- **Data skew or uneven tasks:** If one task handles a disproportionately large chunk of data (skew), it may spill while other tasks do not. Mitigating skew (e.g., by repartitioning or salting keys) can reduce spills ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=,fit%20your%20nature%20of%20data)).

In summary, **Spark spills data to disk when available memory is insufficient for the task’s data** ([Understanding Apache Spark’s Memory Architecture for Efficiency](https://statusneo.com/understanding-apache-sparks-memory-for-better-efficiency/#:~:text=,storage%20memory%20can%20reclaim%20any)). This can happen due to large partitions or not enough memory allocated for execution. Next, we examine how Spark’s memory management determines the amount of memory available for execution and storage, and thus influences spilling.

## Spark Memory Management Internals and Spilling

Spark employs a *unified memory management model* that divides an executor’s JVM heap into several regions: a **reserved region**, **user memory**, and **Spark memory**, which is further split between **execution** and **storage**. The diagram below illustrates this structure ([Spark Memory Management | Distributed Systems Architecture](https://0x0fff.com/spark-memory-management/1)). 

 ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=,to%20being%20evicted%20by%20execution)) ([Spark Memory Management | Distributed Systems Architecture](https://0x0fff.com/spark-memory-management/#:~:text=which%20would%20consume%20so%20called,code%20might%20cause%20OOM%20error))*Spark’s JVM heap is divided into: a fixed reserved region (purple, ~300MB by default), user memory (blue), and “Spark memory” (red). `spark.memory.fraction` (default 0.6) controls what fraction of heap (after reserving 300MB) is used as Spark memory for execution + storage ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=,to%20being%20evicted%20by%20execution)). Within Spark memory, `spark.memory.storageFraction` (default 0.5) defines the boundary (dashed line) between storage and execution regions ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=and%20storage%20share%20a%20unified,due%20to%20complexities%20in%20implementation)) ([The Guide To Apache Spark Memory Optimization](https://www.unraveldata.com/apache-spark-and-memory/#:~:text=However%2C%20there%20is%20no%20static,is%20never%20evicted%20by%20storage)). Execution and storage share this space dynamically.* ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=User%20Memory%20%3D%20,1%20GBs)) ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=From%20all%20sections%20of%20memory,is%20called%20%E2%80%9CDynamic%20Occupancy%20Mechanism%E2%80%9D))

**Reserved Memory (≈300MB):** Spark *always* leaves a portion of the heap untouched by Spark’s own memory manager ([Spark Memory Management | Distributed Systems Architecture](https://0x0fff.com/spark-memory-management/#:~:text=1,it%20would%20store%20lots%20of)). This is to safeguard the system – it stores internal metadata and provides a buffer to avoid OOM. It’s a fixed 300MB (by default) deducted from each executor’s heap and the driver’s heap ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=,to%20being%20evicted%20by%20execution)).

**User Memory:** After reserving 300MB, the next portion of heap is *user memory*, which is **1 - spark.memory.fraction** of the remaining heap ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=,the%20default%20value%20is%20recommended)) ([Spark Memory Management | Distributed Systems Architecture](https://0x0fff.com/spark-memory-management/#:~:text=which%20would%20consume%20so%20called,code%20might%20cause%20OOM%20error)). By default `spark.memory.fraction = 0.6`, so 40% of (heap-300MB) is user memory. This region is not explicitly managed by Spark; it’s intended for user code and data structures (e.g., Python objects in a UDF, data frames in Pandas UDFs, etc.) ([Spark Memory Management | Distributed Systems Architecture](https://0x0fff.com/spark-memory-management/#:~:text=larger%20heap%20size%E2%80%9D%20error%20message,And%20again)). Spark does **not** account for or spill user memory – if your code uses too much of this region, it can cause OOM errors ([Spark Memory Management | Distributed Systems Architecture](https://0x0fff.com/spark-memory-management/#:~:text=spark,code%20might%20cause%20OOM%20error)). The relatively large default (40%) is to accommodate user objects and any inaccuracies in size estimation ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=,the%20default%20value%20is%20recommended)). If `spark.memory.fraction` is set higher (leaving less user memory), more of the heap is given to Spark’s pools, but there’s greater risk that user code or internal overhead could trigger OOM. Conversely, a lower fraction leaves more safety margin at the cost of more frequent spills ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=,the%20default%20value%20is%20recommended)).

**Spark Memory (Unified)**: This is the **managed region** of heap that Spark uses for caching and execution. It is `spark.memory.fraction` of (heap - 300MB). By default, that’s 60% of the heap after reserving 300MB ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=,to%20being%20evicted%20by%20execution)). Spark Memory is **split into Storage and Execution subsections**, but importantly, the split is *dynamic*. The parameter `spark.memory.storageFraction` (default 0.5) designates a subregion within Spark memory that is reserved for storage (cached data) that **cannot be evicted beyond a point** ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=and%20storage%20share%20a%20unified,due%20to%20complexities%20in%20implementation)) ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=,to%20being%20evicted%20by%20execution)). In other words, `spark.memory.storageFraction` of Spark memory is the guaranteed *minimum* for storage, often noted as region **R**, and the remainder is available for execution by default ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=and%20storage%20share%20a%20unified,due%20to%20complexities%20in%20implementation)). At default settings, this means out of an executor’s heap: 300MB reserved, 40% user, and the 60% Spark memory is split 50/50 (so 30% heap for execution, 30% for storage by default) ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=User%20Memory%20%3D%20,1%20GBs)). For example, with a 10 GB executor heap: ~300MB reserved, ~3.88GB user, ~6GB Spark memory (3GB execution, 3GB storage by default) ([Understanding Apache Spark’s Memory Architecture for Efficiency](https://statusneo.com/understanding-apache-sparks-memory-for-better-efficiency/#:~:text=For%20example%2C%20if%20an%20executor,assuming%20default%20settings)).

**Execution vs Storage Memory:** *Execution memory* is used for short-lived data needed during computations – e.g., shuffle buffers, sort data, hash table for aggregations, etc. ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=Memory%20usage%20in%20Spark%20largely,In%20Spark%2C%20execution)). *Storage memory* is used for longer-lived data like cached RDD or DataFrame partitions, broadcast variables, and any data that needs to be kept for reuse ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=Memory%20usage%20in%20Spark%20largely,In%20Spark%2C%20execution)). Under Spark’s unified memory manager, these two share the same pool and can borrow from each other **with a priority for execution** ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=and%20storage%20share%20a%20unified,due%20to%20complexities%20in%20implementation)) ([Understanding Apache Spark’s Memory Architecture for Efficiency](https://statusneo.com/understanding-apache-sparks-memory-for-better-efficiency/#:~:text=,storage%20memory%20can%20reclaim%20any)). Specifically, execution can steal storage memory when needed (by evicting cached blocks), while storage cannot force eviction of execution data structures ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=and%20storage%20share%20a%20unified,due%20to%20complexities%20in%20implementation)) ([The Guide To Apache Spark Memory Optimization](https://www.unraveldata.com/apache-spark-and-memory/#:~:text=However%2C%20there%20is%20no%20static,is%20never%20evicted%20by%20storage)). This means if you cache a dataset and also run heavy shuffles, Spark will evict (drop) some cached data (potentially allowing it to be recomputed or spilled to disk if cached with MEMORY_AND_DISK) to free space for execution, **until storage usage shrinks to the reserve R (storage fraction * Spark memory) floor** ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=and%20storage%20share%20a%20unified,due%20to%20complexities%20in%20implementation)). **Execution memory thus has higher priority** – it will use as much of the unified pool as needed (up to 100% of Spark memory) when running tasks, spilling cached blocks or evicting them if necessary ([Understanding Apache Spark’s Memory Architecture for Efficiency](https://statusneo.com/understanding-apache-sparks-memory-for-better-efficiency/#:~:text=,storage%20memory%20can%20reclaim%20any)). Storage memory is whatever remains free or is reserved; it can grow into the execution region only when execution is not using it. If no data is cached, the entire Spark memory pool is available for execution, which helps *avoid unnecessary disk spills* ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=This%20design%20ensures%20several%20desirable,how%20memory%20is%20divided%20internally)).

**Dynamic Memory Sharing and Spills:** Because execution and storage share memory, Spark can adapt to different workloads. For instance, an application not using caching can use nearly all (60%) of executor heap for execution needs ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=This%20design%20ensures%20several%20desirable,how%20memory%20is%20divided%20internally)). Conversely, an application heavily caching data can still use some of that memory for execution when needed by evicting cache down to the reserved level ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=and%20storage%20share%20a%20unified,due%20to%20complexities%20in%20implementation)). However, **if both cached data and execution demands together exceed the total Spark memory (M)**, or if execution alone exceeds its available space, Spark will spill. As one source explains: *“Whenever total data size exceeds usable memory size (sum of execution + storage), this is when the spill occurs.”* ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=Dynamic%20Execution)). In practice, that means if you have a lot of cached data occupying memory and a large shuffle happens, execution will evict cached blocks until storage is at the R threshold; if it still needs more space, it has no choice but to start spilling intermediate data to disk ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=and%20storage%20share%20a%20unified,due%20to%20complexities%20in%20implementation)) ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=Dynamic%20Execution)). 

It’s important to note that **Spark’s memory manager only tracks certain operations** (like shuffle, sort, cache) that use its managed memory. Other memory usage (e.g., large Java objects created in a map function) goes to user memory and isn’t subject to spilling by Spark ([Spark Memory Management | Distributed Systems Architecture](https://0x0fff.com/spark-memory-management/#:~:text=which%20would%20consume%20so%20called,code%20might%20cause%20OOM%20error)). If user code uses too much memory, it can lead to executor OOM errors rather than Spark spills. Thus, *spilling is specifically tied to Spark’s managed execution/storage memory.* The `spark.memory.fraction` is set by default to balance these concerns – *increasing it gives more room for Spark-managed data (reducing spills and cache evictions) but leaves less headroom for user objects and may risk OOM*, whereas *decreasing it does the opposite (more headroom, but more frequent spills)* ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=,the%20default%20value%20is%20recommended)) ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=,sparse%20and%20unusually%20large%20records)). The Spark docs note: *“The lower [spark.memory.fraction] is, the more frequently spills and cached data eviction occur.”* ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=,the%20default%20value%20is%20recommended)).

**Memory Management Example:** Suppose an executor has 14 GB heap. With defaults (`fraction=0.6`, `storageFraction=0.5`): about 8.4 GB is Spark-managed memory, split into ~4.2 GB execution and ~4.2 GB storage at the threshold ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=User%20Memory%20%3D%20,1%20GBs)). If you cache a dataset that fills, say, 3 GB, you have ~1.2 GB of storage memory left free (up to the 4.2 GB limit) and ~4.2 GB for execution. If an operation needs 5 GB for shuffle, Spark can take the unused 1.2 GB from storage (evicting some cached data if it were used) to reach ~5.4 GB for execution, but beyond that it cannot evict further without dropping below the 4.2 GB reserved for storage. At that point, any additional data must be spilled to disk. On the other hand, if no data was cached, the full ~8.4 GB could be used for execution, likely avoiding a spill in that scenario. This dynamic mechanism (sometimes called **“dynamic occupancy”** ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=From%20all%20sections%20of%20memory,is%20called%20%E2%80%9CDynamic%20Occupancy%20Mechanism%E2%80%9D)) or **“unified memory management”**) gives flexibility: execution and storage can borrow from each other within the unified pool, but not exceed the total.

**Task-Concurrency and Memory:** Another internal factor impacting spills is how memory is divided among concurrent tasks in an executor. If an executor has multiple cores and runs tasks in parallel, those tasks share the execution memory pool. Spark does not rigidly partition the execution memory per task; instead, tasks allocate from the common pool as needed. However, effectively each running task can use at most a fraction of the pool. For example, if an executor with 3 cores has 360MB of execution memory total, and it runs 3 tasks concurrently, each task might be able to use ~120MB before triggering spills ([The Guide To Apache Spark Memory Optimization](https://www.unraveldata.com/apache-spark-and-memory/#:~:text=Expansion%20Rate%20of%202,360MB%20%2F%203%20%3D%20120MB)) ([The Guide To Apache Spark Memory Optimization](https://www.unraveldata.com/apache-spark-and-memory/#:~:text=Execution%20Memory%20per%20Task%20%3D,360MB%20%2F%203%20%3D%20120MB)). A single task could potentially use more if the others aren’t using their share, but if all need memory, they compete. This is why having fewer cores per executor can sometimes reduce spilling – each task then has a larger slice of memory available (because fewer tasks share the same pool at once) ([The Guide To Apache Spark Memory Optimization](https://www.unraveldata.com/apache-spark-and-memory/#:~:text=size%20of%20such%20a%20String,360MB%20%2F%203%20%3D%20120MB)) ([pyspark - Spark- Spill disk and Spill memory problem - Stack Overflow](https://stackoverflow.com/questions/77532248/spark-spill-disk-and-spill-memory-problem#:~:text=Plz%20read%20carefully,of%20cores%20but%20reduce%20them)). We will see this in tuning strategies.

In summary, Spark’s memory management directly affects spilling: the **size of the execution memory pool** (influenced by executor heap size and spark.memory.fraction) and **how much of it is free from cached data** (influenced by spark.memory.storageFraction and caching behavior) are key. Next, we distinguish how executor vs driver memory play into this.

## Executor Memory vs Driver Memory (and Spills)

In a Spark application, the **driver** and **executors** are separate processes (except in local mode). **Executor memory** is the memory allocated to each executor JVM where tasks run, and it’s where almost all spills occur. **Driver memory** is the memory for the driver process, which coordinates the tasks and may collect or broadcast data but *does not directly perform the bulk of data processing*.

**Executors and Spills:** Each executor has its own memory, governed by `spark.executor.memory` (heap size) plus some overhead memory settings. Spills happen on executors during tasks. If an executor’s tasks can’t keep data in memory (as described above), that executor will spill to disk. Increasing an executor’s memory generally makes spills less likely (since the execution pool grows proportionally) ([pyspark - Spark- Spill disk and Spill memory problem - Stack Overflow](https://stackoverflow.com/questions/77532248/spark-spill-disk-and-spill-memory-problem#:~:text=I%20increased%20the%20executor%20configuration,This%20is%20strange)). Also, adjusting the fraction of that memory devoted to Spark’s use (via `spark.memory.fraction`) changes how much execution memory is available. For example, an executor with 8 GB heap at default settings has ~4.5 GB for Spark memory (after 300MB reserve and 40% user memory), whereas with fraction=0.8 it would have ~7.4 GB for Spark memory – meaning significantly more room for execution and storage before spilling (but only ~15% user memory left). Thus, executor memory and Spark’s memory fraction together determine the threshold at which an executor’s tasks must spill.

**Driver Memory:** The driver’s memory (`spark.driver.memory`) is mostly used for** scheduling, task results, and any data the driver program holds** (like variables in the driver application, or data collected to the driver). The driver does not perform partition-level operations itself in cluster mode – executors do that. Therefore, driver memory being low won’t cause spills in the executors. However, if you call an action like `collect()` or `take()` that brings a large dataset back to the driver, then the driver can run out of memory (this is a driver OOM, not a spill – the driver will crash if it can’t accommodate the data) ([bigdata - what is driver memory and executor memory in spark? - Stack Overflow](https://stackoverflow.com/questions/55274021/what-is-driver-memory-and-executor-memory-in-spark#:~:text=The%20one%20responsible%20to%20handle,too%20much%20data%20to%20it)) ([bigdata - what is driver memory and executor memory in spark? - Stack Overflow](https://stackoverflow.com/questions/55274021/what-is-driver-memory-and-executor-memory-in-spark#:~:text=execute%20your%20job,files%20that%20you%20will%20read)). To avoid driver OOM, either avoid collecting too much data or increase `spark.driver.memory` when needed ([Apache Spark memory configuration with PySpark - Stack Overflow](https://stackoverflow.com/questions/72764458/apache-spark-memory-configuration-with-pyspark#:~:text=Most%20of%20the%20computational%20work,is%20transferred%20to%20Spark%20driver)) ([Apache Spark memory configuration with PySpark - Stack Overflow](https://stackoverflow.com/questions/72764458/apache-spark-memory-configuration-with-pyspark#:~:text=Driver%20memory%20can%20be%20configured,spark.driver.memory)). 

**Influence on Spills:** In cluster deployments, the driver and executors are separate, so driver memory does *not* directly influence executor spilling. Spills are an executor-side phenomenon tied to executor memory. One exception is **local mode** or Spark standalone with deploy-mode=client on a single machine: in such cases, the driver and executors may share the same JVM (for example, `master=local[*]` uses one JVM for everything). In local mode, `spark.driver.memory` effectively also limits the memory for execution because the driver process is doing the work of an executor. So in local mode, a spill can occur if the driver (acting as executor) exhausts its allocated heap. In cluster mode, the driver could be relatively small (e.g., 2-4 GB is common) since it’s not doing heavy data shuffles itself ([bigdata - what is driver memory and executor memory in spark? - Stack Overflow](https://stackoverflow.com/questions/55274021/what-is-driver-memory-and-executor-memory-in-spark#:~:text=The%20one%20responsible%20to%20handle,too%20much%20data%20to%20it)) – giving more driver memory won’t reduce executor spills, but it can prevent driver-side issues (like OOM when collecting results or handling large job plans).

**Summary:** Allocate sufficient memory to executors to hold shuffle data to minimize spills. Driver memory should be sized for the job coordination and any data the driver needs to handle (like collected results or broadcast variables), but it doesn’t help with shuffle spills. It’s also important to allocate enough **overhead memory** (off-heap) if using off-heap or running on Yarn (YARN mandates an off-heap overhead) – e.g., `spark.executor.memoryOverhead` – but the core spilling logic concerns the on-heap Spark memory region. 

Next, we’ll demonstrate spills with PySpark examples in both local (standalone) and cluster modes, and then discuss tuning parameters to mitigate spills.

## Examples of Spills in PySpark (Standalone vs. Cluster Mode)

To **observe Spark spilling in action**, you can intentionally create a scenario where an operation’s data exceeds memory. Below are illustrative PySpark examples – one running in local mode (standalone) and one on a cluster – that trigger spills, along with how to detect the spill and then adjust memory settings to alleviate it.

### Example 1: Forcing a Spill in Standalone (Local) Mode

In this example, we use PySpark in local mode with a small memory setting to provoke a spill during a large sort. We’ll also set a low `spark.memory.fraction` to further constrain the execution memory. 

**Setup:** We create a Spark session with limited memory (e.g., 512 MB) and override `spark.memory.fraction` to 0.2 (20%) to leave very little execution space. Then we generate an RDD of a few million integers and perform a costly operation (such as sorting all the data by key). The code might look like:

```python
from pyspark.sql import SparkSession

# Start a Spark session in local mode with small memory
spark = SparkSession.builder \
    .master("local[1]") \
    .appName("SpillDemoLocal") \
    .config("spark.driver.memory", "512m") \
    .config("spark.memory.fraction", "0.2") \
    .getOrCreate()

sc = spark.sparkContext
sc.setLogLevel("INFO")  # to see spill logs in console

# Create an RDD with 5 million integers and one partition (to make it heavy)
rdd = sc.parallelize(range(5000000), numSlices=1)
# Transform it into key-value pairs and sort by key (forces shuffle + sort)
sorted_rdd = rdd.map(lambda x: (x, 1)).sortByKey()  # sorting 5 million elements

# Action to trigger computation
sorted_rdd.count()
```

In this code, we used `master("local[1]")` to run one task at a time (one core) – this maximizes the memory each task can use (since only one task will use the 0.2 * 512MB ~ 102MB of Spark memory at a time). However, even 5 million integers (in Python, as PySpark uses pickled Python objects) likely cannot sort entirely in ~100MB, so Spark should spill during the sort. We also set the RDD to a single partition intentionally to concentrate the data (making the partition ~5 million elements large).

**Detecting the Spill:** When you run this, you should see **log messages** on the console indicating spilling. Spark logs messages like: 

```
INFO ExternalSorter: Spill occurred...
INFO ExternalSorter: Spilling in-memory sorted data of 95.3 MB to disk (1 time so far)
```

Or for aggregate spills, messages like: 

```
INFO ExternalAppendOnlyMap: Spilling map of 1000000 records (approx 120.0 MB) to disk (1 time so far)
```

These logs tell you that Spark has started writing data to disk because the in-memory data reached ~95 MB (close to our 102MB limit) and couldn’t grow further. In the Spark UI, if you check the stage details for the sort operation, you would see non-zero “Shuffle spill (memory)” and “Shuffle spill (disk)” metrics ([The Guide To Apache Spark Memory Optimization](https://www.unraveldata.com/apache-spark-and-memory/#:~:text=partitions%20should%20be%20used%20is,clicking%20on%20its%20Description%20entry)) ([Web UI - Spark 3.5.3 Documentation](https://spark.apache.org/docs/3.5.3/web-ui.html#:~:text=,of%20the%20data%20on%20disk)). For example, it might show something like **Shuffle spill (memory): 100 MB, Shuffle spill (disk): 50 MB**, meaning at the time of spilling, 100 MB was in memory and 50 MB ended up on disk (Spark reports these separately ([Web UI - Spark 3.5.3 Documentation](https://spark.apache.org/docs/3.5.3/web-ui.html#:~:text=,of%20the%20data%20on%20disk))).

**Understanding the Behavior:** In this scenario, with `spark.memory.fraction=0.2`, only 20% of (512-0.3GB) is Spark memory (~102MB). The single task sorting ~5 million records quickly filled its ~100MB execution memory. Spark’s sorter then wrote out a chunk of data to disk (spilled) to free memory and continued the sort in multiple passes (external merge sort). If we did not spill, the job would likely run out of memory and fail.

### Example 2: Spill in Cluster Mode (Multiple Executors)

Now consider a cluster mode example. Suppose we have a cluster with executors of 4 GB each. We’ll use a larger dataset and a `reduceByKey` operation to trigger aggregator spills. In cluster mode, we don’t directly set `spark.driver.memory` for executor processes; instead we configure `spark.executor.memory` and let Spark allocate multiple executors. We also illustrate how multiple tasks can spill.

**Setup:** We’ll simulate a dataset of key-value pairs (to stress a hash aggregation) and intentionally use few partitions to make some partitions huge. For demonstration, we assume we submit this with 2 executors, each 4 GB heap, default memory fraction (0.6). We’ll tweak `spark.memory.storageFraction` to reserve more space for execution if needed.

PySpark code (to be run via `spark-submit` or similar on a cluster):

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("SpillDemoCluster") \
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.instances", "2") \
    .config("spark.memory.storageFraction", "0.3") \
    .getOrCreate()
sc = spark.sparkContext

# Create an RDD of 100 million keys (0 to 999) to force huge aggregations per key
# Each key repeats 100,000 times to make values list large.
import random
NUM_KEYS = 1000
PARTS = 2  # only 2 partitions to force large partition size
data = [(random.randint(0, NUM_KEYS-1), 1) for _ in range(100_000_000)]
rdd = sc.parallelize(data, PARTS)

# Use reduceByKey to sum values for each key (forces aggregation and shuffle)
result = rdd.reduceByKey(lambda a,b: a+b)
result.foreach(lambda x: None)  # action to execute the reduceByKey
```

In this scenario, we have 2 partitions (and 2 executors, so one partition per executor). Each partition holds ~50 million (key,1) pairs. The keys range only 0 to 999, so each executor will accumulate huge counts for each of 1000 keys. The in-memory combiners (Spark uses an `ExternalAppendOnlyMap` for reduceByKey) will grow as it receives more records. We set `spark.memory.storageFraction=0.3` which means of the ~ (0.6 * 4GB ≈ 2.2GB) Spark memory, only 30% (~0.66GB) is reserved for storage and 70% (~1.54GB) is initially for execution. We assume we didn’t cache anything significant, so essentially each task can use up to ~2.2GB for execution (taking from storage if unused). Still, 50 million records (even as integers) with the overhead of Python objects or even just the Java overhead for the map likely exceeds that.

**Detecting spills on the cluster:** We won’t see the logs in the console (since executors log to their own stdout or to cluster log files), but we can open the **Spark UI** for the application. Under the **Stages** tab, find the stage corresponding to the `reduceByKey` shuffle. There you will see metrics like *Shuffle spill (memory)* and *Shuffle spill (disk)* for that stage. It might show something like each task spilled several GB. For instance, each executor might report **Shuffle spill (memory): 1.5 GB, Shuffle spill (disk): 1.0 GB**, indicating it kept 1.5 GB in memory but had to spill an extra 1.0 GB to disk for the aggregation. The Spark UI’s “Tasks” table would also show for each task if it spilled (and how many records). In our case, with one task per executor, both tasks likely spilled. The **“Peak Execution Memory”** metric in the UI might also show around the maximum used (which would be ~ the execution pool size) ([Web UI - Spark 3.5.3 Documentation](https://spark.apache.org/docs/3.5.3/web-ui.html#:~:text=from%20workers.%20,bytes%20read%20from%20remote%20executors)).

If we check the executor logs (e.g., in Yarn or standalone worker logs), we would find lines similar to: 

```
INFO ExternalAppendOnlyMap: Spilled in-memory map of 50000000 elements (approx 1.6 GB) to disk (2 times so far)
```

Meaning the reduce task spilled the data structure to disk multiple times. This corresponds with Spark breaking the aggregation into multiple batches because it couldn’t hold all 50 million entries in memory at once.

**Adjusting memory to avoid the spill:** If we re-run this job with more memory or different settings, we can reduce or eliminate the spilling. For example, if we double the executor memory to 8 GB, each executor’s execution memory roughly doubles (to ~4.4 GB with default fraction), which might handle the 50 million records without spilling. Indeed, in one user’s case, increasing executors from 14GB to 56GB eliminated spills for their join workload ([pyspark - Spark- Spill disk and Spill memory problem - Stack Overflow](https://stackoverflow.com/questions/77532248/spark-spill-disk-and-spill-memory-problem#:~:text=I%20increased%20the%20executor%20configuration,This%20is%20strange)). Alternatively, we could increase `spark.memory.fraction` to use more of the 4 GB heap for Spark (less for user memory). If we set `spark.memory.fraction=0.8` for the 4 GB executor, Spark memory becomes ~ (0.8 * (4096-300)MB) ≈ 3037MB, leaving only ~759MB for user memory. That ~3GB Spark memory (versus ~2.2GB before) might be enough to hold the structure without spilling. 

Another approach is reducing the number of keys per task by increasing partitions. If we used 50 partitions instead of 2, each task would handle only ~2 million records, likely fitting in memory. This is a trade-off between parallelism and per-task data size.

**Summary of examples:** In local mode, we saw how a constrained memory setting causes spills and how to identify them via logs. In cluster mode, we simulated a heavy shuffle that spills and would inspect the Spark UI metrics to quantify it. In both cases, adjusting memory settings (either giving more memory or tuning fractions) can change spilling behavior. Next, we’ll outline general **memory tuning strategies** to reduce or avoid spills in Spark.

## Memory Tuning Strategies to Reduce Spills

Optimizing Spark’s memory settings and data distribution can **significantly reduce disk spills**, improving performance. Here are several strategies:

- **Increase Executor Memory:** The simplest way to reduce spills is to give executors more heap memory (`spark.executor.memory`). A larger heap means a larger Spark memory pool for execution. As shown earlier, doubling an executor’s memory can eliminate spills if memory was the only bottleneck ([pyspark - Spark- Spill disk and Spill memory problem - Stack Overflow](https://stackoverflow.com/questions/77532248/spark-spill-disk-and-spill-memory-problem#:~:text=I%20increased%20the%20executor%20configuration,This%20is%20strange)). Be mindful of diminishing returns – very large heaps can lead to longer garbage collection pauses. Also ensure the cluster has enough RAM to accommodate the executors.

- **Adjust `spark.memory.fraction`:** This controls what fraction of the executor’s heap is used for Spark’s execution+storage memory (the rest is “user” memory) ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=,to%20being%20evicted%20by%20execution)). The default 0.6 is conservative to avoid OOM in most cases ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=,the%20default%20value%20is%20recommended)). **Increasing `spark.memory.fraction` (e.g., to 0.7 or 0.8)** gives Spark more room for caching and shuffle, which can directly reduce spills ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=,the%20default%20value%20is%20recommended)). For example, going from 0.6 to 0.8 on a 16 GB executor increases Spark-managed memory from ~9.4 GB to ~12.5 GB – meaning ~3 GB more for execution/storage. This could turn a spill scenario into in-memory. **However, caution:** by leaving only 20% for user memory, you risk OOM if your tasks or data structures need a lot of off-record memory (e.g., large Python objects, huge task results, etc.). It’s often wise to change this only if you have profiled memory usage or observed that user memory isn’t heavily used. Spark’s docs even note that leaving it at default is usually recommended unless you have a specific reason ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=,the%20default%20value%20is%20recommended)). If you do raise it, test carefully.

- **Adjust `spark.memory.storageFraction`:** This controls the *minimum* fraction of Spark memory reserved for storage (cached data) ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=,to%20being%20evicted%20by%20execution)). By default it’s 0.5, meaning Spark tries to balance execution and storage. If your application is execution-heavy and you don’t need to cache much data, you can **lower `spark.memory.storageFraction`** to, say, 0.3. This means only 30% of Spark memory is reserved for storage, and up to 70% can be used by execution before it starts evicting cached data. A lower storage fraction gives execution more headroom (reducing spills during big shuffles) at the expense of caching less data without eviction. On the other hand, if your job relies on heavy caching and you can tolerate some spilling in execution, you might keep this higher. Note that this fraction **does not strictly partition the memory** – it just sets the eviction policy threshold ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=and%20storage%20share%20a%20unified,due%20to%20complexities%20in%20implementation)). For example, with storageFraction=0.3, execution can evict storage blocks until cached RDD data is only using 30% of Spark memory. Beyond that, execution cannot steal more, so if execution still needs memory it will spill. In practice, many jobs don’t need to change this, but it’s a lever if you know your workload skew (e.g., mostly computation vs. heavy caching).

- **Reduce Concurrent Tasks per Executor:** As discussed, if an executor runs many tasks in parallel (many cores), they share the execution pool and can force each other to spill. **Using fewer cores per executor** (and thus more executors) can sometimes improve memory usage per task. For instance, instead of 1 executor with 8 cores and 8 GB, you might use 2 executors with 4 cores and 4 GB each. The total memory is the same, but now each executor’s 4 cores share 4 GB (1 GB per core, roughly) instead of 8 cores sharing 8 GB (0.5 GB per core). This can help each task have a larger slice of memory, possibly avoiding spills ([The Guide To Apache Spark Memory Optimization](https://www.unraveldata.com/apache-spark-and-memory/#:~:text=Expansion%20Rate%20of%202,360MB%20%2F%203%20%3D%20120MB)) ([pyspark - Spark- Spill disk and Spill memory problem - Stack Overflow](https://stackoverflow.com/questions/77532248/spark-spill-disk-and-spill-memory-problem#:~:text=Plz%20read%20carefully,of%20cores%20but%20reduce%20them)). The downside is more executors means more overhead and possibly more shuffle data exchange across executors, but if memory was the bottleneck, this can be beneficial.

- **Enable Off-Heap Memory:** Spark can use off-heap memory for execution/storage if configured (`spark.memory.offHeap.enabled=true` and set `spark.memory.offHeap.size`) ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=task%20and%20storage%20task,is%20when%20the%20spill%20occurs)). Off-heap memory is not subject to JVM GC in the same way and can reduce GC overhead. While off-heap doesn’t magically increase the total memory, it can supplement on-heap storage. For example, you might give each executor 8 GB heap and 4 GB off-heap. Spark will then use that extra 4 GB (outside the JVM heap) for execution/storage as well. If you were previously spilling with 8 GB, the 4 GB off-heap might now hold the data. This is more advanced and typically used in Spark SQL/Tungsten in-memory format, but it’s an option to reduce spills if GC or JVM overhead was limiting effective memory usage ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=task%20and%20storage%20task,is%20when%20the%20spill%20occurs)).

- **Data Partitioning and Shuffle Optimization:** Beyond memory configs, you can often reduce spills by changing how data is partitioned:
  - Increase the number of partitions for heavy shuffles (`spark.sql.shuffle.partitions` for DataFrames or manually `rdd.repartition()` for RDDs) so each task has less data to process ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=,fit%20your%20nature%20of%20data)). Smaller partitions mean each task’s memory demand is lower, less likely to spill (though too many partitions have their own overhead).
  - Avoid extreme skew: if one or few keys are very large (causing one task to handle a huge partition), consider techniques to mitigate skew (salting keys, using `reduceByKeyLocally` for very hot keys, etc.) ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=,fit%20your%20nature%20of%20data)). Skew is a common cause of spills – solving skew can eliminate the worst spills.
  - Use combiners or aggregate earlier: For example, using `combineByKey`/`reduceByKey` instead of `groupByKey` drastically reduces memory needs by combining values incrementally. A `groupByKey` holds all values for a key in memory, which is much more memory-intensive than `reduceByKey` which merges as it goes. If you accidentally use a non-combined aggregation, switching to a combined version can avoid large in-memory buffers (thus avoiding spills or even OOM).
  - Limit input partition size: on file reads, Spark by default uses `spark.sql.files.maxPartitionBytes` (128 MB by default for Parquet/ORC) or splits text files such that partitions are usually <= 128 MB. If you have a higher setting or huge input splits, consider lowering it so that one partition (and thus one task) doesn’t read an enormous chunk ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=,bigger%20file%20sizes%20per%20partition)). Smaller partition bytes -> more partitions -> less memory per task.
  
- **Proper Caching Strategy:** If you cache large datasets, use the right storage level. For example, use `MEMORY_AND_DISK` instead of `MEMORY_ONLY` if the dataset might not fit in memory. That way Spark will spill cache partitions to disk if needed (this is a different kind of spill – caching spill, not execution spill). This prevents your cache from consuming all storage memory and pushing the execution to spill. Also, beware of caching data you don’t need or leaving cached RDDs around – free up memory by unpersisting when caches are no longer needed, to give more room to execution.

- **Compression and Serialization:** Enabling shuffle compression (`spark.shuffle.compress=true`, on by default) and using efficient serializers (Kryo vs Java serialization) reduces the memory footprint of data structures, which can indirectly reduce spills by fitting more data in memory ([Tuning - Spark 3.5.5 Documentation - Apache Spark](https://spark.apache.org/docs/latest/tuning.html#:~:text=This%20guide%20will%20cover%20two,memory%20use%2C%20and%20memory%20tuning)). For example, a bulky Java object might occupy 10x more space in memory than its serialized form; using Spark’s Tungsten binary format or Kryo-optimized classes can allow more data to reside in memory before spilling. The trade-off is CPU overhead for compression, but it’s usually worth it when memory is tight.

**Tuning in Practice:** Often, the first steps to reduce spills are: **increase executors memory** (if possible), **increase spark.memory.fraction modestly** (if you observe a lot of Spark-managed memory spilling and not much else using memory), and **ensure your partitions are reasonable** in size. For example, if your Spark UI shows each task spilling ~500 MB to disk, try doubling the executor memory or cutting the partition size in half (doubling partitions) – either approach might cut the spill or eliminate it.

Let’s revisit our examples with tuning:

- In the local sort example, setting `spark.memory.fraction` back to 0.6 or increasing driver memory to 1g might have prevented the spill for 5 million integers. Alternatively, using `parallelize(range(5000000), 4)` to create 4 partitions (so each sorts 1.25 million) could keep each partition sort within the ~100MB limit, avoiding spill. 
- In the cluster reduceByKey example, using 8 GB executors or increasing fraction to 0.8 likely avoids spilling. Or simply increasing partitions from 2 to 20 would mean each executor handles ~5 million records per partition instead of 50 million, likely fitting in the ~2.2GB execution memory without spill. 

Each solution has cost implications (more memory = more resources; more partitions = possibly more shuffle overhead but less spill). The optimal tuning often requires understanding your data characteristics and sometimes a bit of trial and error with Spark’s metrics as feedback.

## Conclusion

**Spark spills to disk when executors run out of execution memory** for intermediate data. This behavior, while preventing outright failures, can severely degrade performance. We examined how Spark’s unified memory management – dividing memory into execution and storage regions within each executor – influences the frequency of spills. Key points include the interplay of `spark.memory.fraction` (determining total heap for Spark) and `spark.memory.storageFraction` (soft boundary between cache and execution memory) ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=,to%20being%20evicted%20by%20execution)) ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=and%20storage%20share%20a%20unified,due%20to%20complexities%20in%20implementation)). We also distinguished executor memory (where spills happen) from driver memory (unrelated to spilling except in local scenarios). Through examples in PySpark, we saw how altering memory settings or data partitioning can change spilling behavior, and learned to identify spills via Spark’s UI and logs (look for “spill” metrics or log messages ([The Guide To Apache Spark Memory Optimization](https://www.unraveldata.com/apache-spark-and-memory/#:~:text=partitions%20should%20be%20used%20is,clicking%20on%20its%20Description%20entry)) ([Web UI - Spark 3.5.3 Documentation](https://spark.apache.org/docs/3.5.3/web-ui.html#:~:text=,of%20the%20data%20on%20disk))). 

To optimize Spark applications and **reduce disk spills**, consider increasing executor heap memory, tweaking Spark’s memory fraction (with care), adjusting the storage fraction if appropriate, and improving your data partitioning to avoid huge per-task loads. Also leverage combination operations and mitigate skew to balance the memory load. Remember that the default memory settings are a balance intended for general workloads ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=,the%20default%20value%20is%20recommended)) – any tuning should be guided by actual performance metrics and understanding of your specific job’s memory usage. With informed adjustments, you can often keep more data in RAM and avoid the costly penalty of disk spills, leading to faster and more efficient Spark jobs.

**Sources:**

- Spark Official Documentation – Memory Tuning and Configurations ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=Memory%20usage%20in%20Spark%20largely,not%20evict%20execution%20due%20to)) ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#:~:text=,to%20being%20evicted%20by%20execution)) ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=,the%20default%20value%20is%20recommended))  
- “Spark Performance Tuning: Spill” by Wasurat Soontronchai ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=Simply%20put%2C%20this%20behavior%20occurs,for%20the%20remaining%20tasks%20within)) ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=User%20Memory%20%3D%20,1%20GBs)) ([Spark Performance Tuning: Spill. What happens when data is overload your… | by Wasurat Soontronchai | SelectFrom](https://selectfrom.dev/spark-performance-tuning-spill-7318363e18cb#:~:text=Dynamic%20Execution))  
- Apache Spark Memory Management (Unified) – 0x0fff.com by Alexey Grishchenko ([Spark Memory Management | Distributed Systems Architecture](https://0x0fff.com/spark-memory-management/#:~:text=which%20would%20consume%20so%20called,code%20might%20cause%20OOM%20error)) ([The Guide To Apache Spark Memory Optimization](https://www.unraveldata.com/apache-spark-and-memory/#:~:text=However%2C%20there%20is%20no%20static,is%20never%20evicted%20by%20storage))  
- Spark UI Documentation – Metrics for Spills ([The Guide To Apache Spark Memory Optimization](https://www.unraveldata.com/apache-spark-and-memory/#:~:text=partitions%20should%20be%20used%20is,clicking%20on%20its%20Description%20entry)) ([Web UI - Spark 3.5.3 Documentation](https://spark.apache.org/docs/3.5.3/web-ui.html#:~:text=,of%20the%20data%20on%20disk))  
- Stack Overflow – Various Q&A on Spark spills and memory (e.g., driver vs executor memory, memory fraction) ([bigdata - what is driver memory and executor memory in spark? - Stack Overflow](https://stackoverflow.com/questions/55274021/what-is-driver-memory-and-executor-memory-in-spark#:~:text=The%20one%20responsible%20to%20handle,too%20much%20data%20to%20it)) ([Why is the default value of spark.memory.fraction so low? - Stack Overflow](https://stackoverflow.com/questions/74784139/why-is-the-default-value-of-spark-memory-fraction-so-low#:~:text=,the%20default%20value%20is%20recommended)) ([Apache Spark memory configuration with PySpark - Stack Overflow](https://stackoverflow.com/questions/72764458/apache-spark-memory-configuration-with-pyspark#:~:text=Driver%20memory%20can%20be%20configured,spark.driver.memory)).