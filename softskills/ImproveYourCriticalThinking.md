Improve Your Critical Thinking Skills
Contributed by Charles Humble

Difficulty level: Intermediate

An awful lot of our school education system, at least where I’m from in the UK, is dedicated to teaching people to learn reams of facts and then regurgitate them in exams. While this skill has its uses, we live in an age where almost any “fact” we want can be found on the internet. However, many of those facts are wrong or misleading, intended to overwhelm our critical faculties rather than inform. I would argue that a more useful skill would be to learn how to determine when something is, on the balance of probabilities, likely to be true.

This is harder than it might sound because of confirmation bias—the human tendency to search for, interpret, favor, and recall information in a way that supports our prior beliefs. Confirmation bias is insuperable, but can be managed by being trained in critical thinking skills.

For software engineers, critical thinking is a tool that can help us to innovate and find novel solutions to problems. It also provides us with a means to challenge decisions made solely on the basis of pecking order—we’re building this feature because the founder thinks it is a good idea, or we’re using this framework because the lead developer says it is non-negotiable.

It is a very broad topic, however. In the previous Shortcut in this series, we talked about one aspect—the importance of really understanding a problem, coming up with multiple solutions, comparing them, and then deciding which one to pursue. You may have noticed I skipped rather lightly over the whole “comparing and deciding which one to use” bit, so in this Shortcut we’re going to tackle that with a discussion on the basics of critical thinking.

Connie Missimer, author of the excellent book Good Arguments: An Introduction to Critical Thinking and a regular trainer on the O’Reilly platform, says critical thinking is all about weighing evidence.

You have probably made decisions at work based on gut instinct. I certainly have. But for a decision that matters, we want to build an evidence base that can help inform us. Everything has a trade-off, of course. I’ve worked in an environment where the amount of evidence required to make a decision was so high that we spent years in analysis and never built anything. I’ve also seen environments where major decisions were made with no supporting evidence at all. Neither is ideal. The trade-off is around finding the right balance between weight and effectiveness—and for our purposes, there is a hierarchy.

The heaviest and most effective method is to run a controlled experiment, such as an A/B test, to eliminate alternatives. It is a fantastic method, but designing good A/B tests is difficult, time-consuming, and costly. You need to get a number of things right at the organizational level because experiments don’t always tell you what you want them to. That is the point, but in an organization where failure is punished, running tests isn’t going to be encouraged. Likewise, there is no point running an experiment if you don’t have a hypothesis you are trying to test.

A second approach is to check your existing data for correlations. This is slightly less time-consuming, but it’s not always obvious why two things are correlated. You may be familiar with the phrase “correlation does not imply causation,” but this does not stop people from drawing causal inferences from correlational statements all the time. In my working life I’ve seen numerous examples of people making assumptions, apparently supported by data, and they are often costly and hard to reverse. Tyler Vigen has an entire website dedicated to finding spurious correlations.

A third option is to look at historical arguments. Ask yourself, has anyone studied this before and, if they have, what conclusions did they reach? Below this in the hierarchy, you have the collective experience of the team, and finally anecdotal evidence that comes from a single observation. Anecdotes are interesting but they are not evidence. That said, a number of the most successful product changes I made when I was InfoQ’s chief editor started as an anecdote from a single conversation with a reader; we replaced video interviews with podcasts before the whole world produced them, because someone told me at a conference that she wanted to consume our content while walking her dog.

Of course, that decision wasn’t made solely on the basis of one conversation. We looked at production costs of one format versus another, and at the consumption patterns for video interviews. We worked with a team to create a podcast format that we felt worked. We set target numbers for the new format, got buy-in to run the experiment, paused video interview production, launched the podcast, and monitored the numbers to see what would happen. It was, though, reasonably straightforward.

Some decisions are, of course, more involved. Suppose you were a VP of engineering for a news website and a reader told you that she was more likely to read news stories if they are illustrated, particularly if the pictures have something a bit fun or interesting about them. She also told you that those images “travel well on social media.” That is an anecdote, so you should listen to it, but it isn’t evidence.

Before committing to hiring a team of in-house artists and implementing the feature, you would, I hope, seek to verify it. If you aren’t sure what our reader means by “travels well on social media,” ask her. What would it cost to add images to every story? What uplift in terms of click-through rate (CTR) would you need to cover that cost? Has anyone looked at this before and, if so, what does that research tell me? We have images on some stories already, so do those stories have a higher CTR than the ones that don’t? Is that because of the pictures, or because we tend to commission images only for stories that we think are particularly strong? How do we know?

While we’re considering this, we should also think about whether CTR is the right metric to be looking at. Is the number of people clicking on stories really what we care about, or do we want to be optimizing for something else, such as an engagement metric? We might use a combination of time on page and scroll depth as a proxy for that.

If the numbers look plausible, you might want to design an experiment. Suppose you calculate that you need a minimum 15% rise in time-on-page to justify the cost. You can segment the traffic so that half your site visitors see an image and the other half do not, then see which population has the higher figure. If the group with images has a significantly higher time-on-page, is it high enough to justify the change? If not, you can run another experiment; do images we’ve created in-house, which have a bit of personality and fun, have a higher dwell time than stock images sourced from Unsplash or Getty?

The wonderful thing about testing like this is that it can go in all sorts of unexpected directions, which is why many of the best organizations conduct so much research. The key point to remember is that it is evidence, rather than preconceptions or seniority, that should be guiding the decisions we make.

There are a few more criteria to keep in mind. First, it can be easy to end up optimizing for elements you can measure, rather than those that have the biggest effect. I would encourage you to read the book How to Measure Anything, by Douglas W. Hubbard, for that reason. Second, sometimes our gut instincts are good, sometimes less so; if you can justify it, maybe try an idea and see if it works.

Finally, do revisit an idea if something changes that might have a bearing on your assumptions. Perhaps in the preceding example, the cost of producing images is too high, even though they do have a measurable impact on engagement. Does the recent rise of generative AI tools allow you to create images that have the same effect at a more palatable price point?

Thinking this way—asking questions, forming a hypothesis, gathering and weighing evidence—is a skill that has remarkably broad applicability. I’ve used it when making investment decisions at an executive level and when designing products and features. I’ve also used it when debugging complex systems, including teams. I’ve used it when evaluating whether to use dependency injection to solve a coding problem or to just write half a dozen factories. And now I use it all the time in my writing and interviewing. It takes practice to get good at it, but it is worthwhile.