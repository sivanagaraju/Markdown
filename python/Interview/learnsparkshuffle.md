Great! I will provide a deep dive into how DataFrames are built on RDDs and how `spark.sql.shuffle.partitions` interacts with `spark.default.parallelism`.

This will cover:
1. **How DataFrames are internally built on RDDs** – including physical plans and execution stages.
2. **How transformations like `groupBy()`, `join()`, and `aggregate()` use different partitioning rules** – and when Spark switches from using RDD’s parallelism to shuffle partitions.
3. **Detailed execution flow of a Spark job with RDDs vs DataFrames** – with examples showing partition behavior and how it affects performance.
4. **Optimization strategies** – tuning `spark.sql.shuffle.partitions` and `spark.default.parallelism` for best performance.

I'll gather detailed insights and get back to you soon!

# Spark SQL/DataFrame Execution and RDD Partitioning

Spark DataFrame (Dataset) operations are ultimately executed using Spark’s RDD infrastructure under the hood. Understanding how Spark translates DataFrame transformations into RDD jobs and how it uses configuration settings for parallelism is key to optimizing performance. Two important configs are **`spark.default.parallelism`** (for RDDs) and **`spark.sql.shuffle.partitions`** (for DataFrame shuffles). Below, we break down the internal execution and how these settings interact, with examples for common operations like groupBy and join.

## 1. Internal Execution Plan: DataFrames to RDDs 
A DataFrame API call in Spark goes through a catalyst **query plan** before execution. When you perform transformations (e.g., `select`, `filter`, `groupBy`) on a DataFrame, Spark builds a **logical plan**, optimizes it into a **physical plan**, and finally executes that plan as RDD operations. The physical plan consists of Spark operators (e.g., filters, hash aggregations) that get executed as tasks on partitions. In fact, when an action is called (like `.collect()` or `.show()`), Spark generates an RDD of internal rows and executes it. The `Dataset.toRdd` operation is the boundary where Spark SQL’s optimized plan is converted to an actual RDD for Spark Core to execute ([QueryExecution - The Internals of Spark SQL](https://books.japila.pl/spark-sql-internals/QueryExecution/#:~:text=The%20%60RDD%60%20is%20the%20top,that%20represent%20physical%20operators)). At that point, you have “left” the DataFrame world and entered the RDD execution world – the DataFrame’s query plan is run as one or more RDD jobs on the cluster. 

**Key point:** *Every Spark DataFrame/Dataset ultimately runs as one or more RDDs under the hood.* Each partition of a DataFrame is an RDD partition (often containing Spark’s `InternalRow` objects), and each Spark task operates on one partition. The planner may use techniques like Whole-Stage Code Generation (which fuse multiple operations into a single optimized function per partition), but it’s still executed per RDD partition. For example, a DataFrame transformation plan might be executed as a chain of RDD map and shuffle steps. You can see this if you call `df.explain()` – the physical plan will include operators and sometimes an `Exchange` node indicating an RDD shuffle. In summary, DataFrame computations are ultimately RDD computations scheduled by Spark.

## 2. Partitioning in DataFrames: `spark.default.parallelism` vs `spark.sql.shuffle.partitions`
Spark uses two separate settings to decide how many partitions (and thus parallel tasks) to use:
- **`spark.default.parallelism`** – the default number of partitions for RDD operations (in Spark Core).
- **`spark.sql.shuffle.partitions`** – the number of partitions for shuffle operations in Spark SQL/DataFrame API.

**`spark.default.parallelism` (RDD parallelism):** This config affects **core RDD transformations**. It determines the default partitions for RDDs when the level of parallelism isn’t explicitly specified. For example:
  - If you call `sc.parallelize()` on a collection without specifying a partition count, it will create that RDD with `spark.default.parallelism` partitions (or based on cluster defaults). In local mode it’s the number of cores; in cluster mode it’s typically the total cores in the cluster or 2, whichever is larger ([Configuration - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/configuration.html#:~:text=,or%202%2C%20whichever%20is%20larger)).
  - For RDD shuffle transformations like `reduceByKey` or `RDD.join` (on Pair RDDs), if you don’t provide a partition number, Spark uses the **largest parent RDD’s partition count** or the default parallelism value ([Configuration - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/configuration.html#:~:text=,depends%20on%20the%20cluster%20manager)). In other words, it tries to maintain or scale up to a reasonable number of partitions for the shuffle. (By default, *“for distributed shuffle operations like reduceByKey and join, [the number of partitions is] the largest number of partitions in a parent RDD”* ([Configuration - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/configuration.html#:~:text=,depends%20on%20the%20cluster%20manager)).) If there’s no parent (e.g., parallelize from scratch), it falls back to the cluster-based default as mentioned. 

**`spark.sql.shuffle.partitions` (DataFrame shuffle parallelism):** This config is used by the Spark SQL engine (DataFrames/Datasets) for the number of partitions after shuffle operations like joins, aggregations, and order-by. By default it’s **200** partitions ([Configuration - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/configuration.html#:~:text=)). Key points:
  - Whenever Spark **shuffles data** for a **join or aggregation** in a DataFrame, it will repartition the data into `spark.sql.shuffle.partitions` partitions by default ([performance - What is the difference between spark.sql.shuffle.partitions and spark.default.parallelism? - Stack Overflow](https://stackoverflow.com/questions/45704156/what-is-the-difference-between-spark-sql-shuffle-partitions-and-spark-default-pa#:~:text=From%20the%20answer%20here%2C%20,data%20for%20joins%20or%20aggregations)) ([Configuration - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/configuration.html#:~:text=)). This acts like the “reducer count” for those operations.
  - This setting does **not** affect non-shuffle operations. If you’re doing column transformations, filters, maps, etc., without requiring a shuffle, it won’t change the partition count.
  - Importantly, `spark.sql.shuffle.partitions` is specific to the high-level DataFrame API. Spark SQL will use this for its own shuffles, **ignoring** `spark.default.parallelism` in those cases ([performance - What is the difference between spark.sql.shuffle.partitions and spark.default.parallelism? - Stack Overflow](https://stackoverflow.com/questions/45704156/what-is-the-difference-between-spark-sql-shuffle-partitions-and-spark-default-pa#:~:text=,ignored%20when%20working%20with%20dataframes)). (Think of DataFrame shuffles as managed by Spark’s SQL planner, which uses its own default of 200 unless overridden.)

**When each is used:** If you are working purely with DataFrame operations (using `Dataset` API or Spark SQL queries), the partitions for shuffles will come from `spark.sql.shuffle.partitions`. In contrast, if you are working with low-level RDDs, the default parallelism for new RDDs or RDD shuffles comes from `spark.default.parallelism` (or the upstream RDD’s partitions). In practice:
- **DataFrame example:** Suppose you load a DataFrame from a file or another source. Initially it might have partitions equal to the number of file splits or some default. If you then do a `df.groupBy("category").count()`, Spark will shuffle and repartition the data into 200 partitions by default (assuming you didn’t change `spark.sql.shuffle.partitions`). The resulting DataFrame after the aggregation will have 200 partitions.
- **RDD example:** If you create an RDD via `rdd = sc.parallelize(data)`, and then do `rdd.reduceByKey(func)` without specifying a partition count, Spark will use either the current `rdd` partition count or `spark.default.parallelism` to decide how many reduce tasks to use ([Configuration - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/configuration.html#:~:text=,depends%20on%20the%20cluster%20manager)). By default, if `data` has no parent RDD, an RDD in cluster mode might default to, say, 8 partitions on an 8-core cluster. A subsequent `reduceByKey` would then also use 8 shuffle partitions (since the upstream had 8). If the upstream RDD had more partitions than the default, it would use that larger number ([Differences between spark.sql.shuffle.partitions and spark.default.parallelism](https://kontext.tech/article/1149/differences-between-sparksqlshufflepartitions-and-sparkdefaultparallelism#:~:text=,or%202%2C%20whichever%20is%20larger)).

**DataFrames ignore `spark.default.parallelism`:** It’s worth noting that `spark.default.parallelism` is mostly irrelevant for the DataFrame API. Spark’s SQL engine does not use it when planning query shuffles ([performance - What is the difference between spark.sql.shuffle.partitions and spark.default.parallelism? - Stack Overflow](https://stackoverflow.com/questions/45704156/what-is-the-difference-between-spark-sql-shuffle-partitions-and-spark-default-pa#:~:text=,ignored%20when%20working%20with%20dataframes)). Instead, it consistently uses `spark.sql.shuffle.partitions` for its shuffles. So, setting `spark.default.parallelism` won’t change the behavior of a DataFrame `join` or `groupBy` – you’d need to adjust `spark.sql.shuffle.partitions` (or explicitly call `.repartition()` on the DataFrame) to change shuffle parallelism ([performance - What is the difference between spark.sql.shuffle.partitions and spark.default.parallelism? - Stack Overflow](https://stackoverflow.com/questions/45704156/what-is-the-difference-between-spark-sql-shuffle-partitions-and-spark-default-pa#:~:text=,ignored%20when%20working%20with%20dataframes)). On the other hand, if you convert a DataFrame to an RDD (using `df.rdd`) or create a DataFrame from an RDD, the initial number of partitions will respect the RDD’s partitioning (which might come from `spark.default.parallelism`). 

**Example (PySpark):** 

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[4]").appName("Example").getOrCreate()
# Set configs for demonstration
spark.conf.set("spark.default.parallelism", "4")
spark.conf.set("spark.sql.shuffle.partitions", "4")

# Create an RDD of 100 numbers with default parallelism (4 partitions)
rdd = spark.sparkContext.parallelize(range(100))
print("RDD partitions:", rdd.getNumPartitions())  # Outputs: RDD partitions: 4

# Convert RDD to DataFrame
df = spark.createDataFrame(rdd.map(lambda x: (x,)), schema=["value"])
print("DataFrame initial partitions:", df.rdd.getNumPartitions())  # Likely 4

# Perform a shuffle operation (aggregation) on the DataFrame
result_df = df.groupBy((df.value % 2).alias("parity")).count()
print("Result DataFrame partitions:", result_df.rdd.getNumPartitions())  # This will be 4 due to spark.sql.shuffle.partitions=4
result_df.show()
```

In the above example, we start with an RDD of 4 partitions (because we set `spark.default.parallelism=4`). After converting to a DataFrame, it still has 4 partitions. But when we do a `groupBy` aggregation (which causes a shuffle), the result is repartitioned into 4 partitions as controlled by `spark.sql.shuffle.partitions`. If we hadn’t set `spark.sql.shuffle.partitions` to 4, it would default to 200 partitions even on this small local job. This illustrates that the DataFrame shuffle uses its own setting, not the RDD’s original partition count. 

Similarly, if you do a DataFrame join between two dataframes, by default Spark will shuffle both sides of the join into 200 partitions (or whatever `spark.sql.shuffle.partitions` is set to), regardless of their original partition counts. We’ll explore this more next.

## 3. Partitioning Behavior in Different Operations
Different transformations have different effects on how partitions are determined, especially between RDD API and DataFrame API. Let’s consider common operations and how Spark decides partition counts:

- **DataFrame/Dataset `groupBy` and Aggregations:** A `groupBy(...).agg(...)` on a DataFrame triggers a **wide transformation**. Spark will perform a **shuffle** to group data by the grouping key. All source partitions’ data is redistributed (hashed by key) into `spark.sql.shuffle.partitions` partitions by default ([Configuration - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/configuration.html#:~:text=The%20default%20number%20of%20partitions,from%20the%20same%20checkpoint%20location)). This means if you had, say, 50 input partitions initially, after a `groupBy` shuffle you’ll end up with 200 output partitions (unless you changed the setting). Each partition becomes a group of certain keys. Internally, Spark’s plan does a **partial aggregation** on each initial partition, then an `Exchange` (shuffle) to hash-partition by key, then a final aggregation per shuffle partition. If you look at `df.groupBy("key").count().explain()`, you’d see an *Exchange* node with something like *hashpartitioning(key, 200)* indicating the 200 shuffle partitions.  
  - *Note:* If you do a global aggregation with no group key (e.g., `df.agg({"column": "sum"})` or `df.count()`), Spark does **not** need to shuffle all data to one partition. Each partition can compute the partial sum/count, and then Spark either uses the driver to sum them up, or uses a single task reduce. In practice, a global `count()` is done very efficiently by just summing the counts from each partition on the driver, avoiding a full shuffle. So not all aggregations use the shuffle partition setting – only those that require redistributing data by key (grouped aggregations).

- **DataFrame `join()`**: Join operations can be of different types (sort-merge join, broadcast join, etc.). By default, if you join two large DataFrames on a key, Spark will shuffle both DataFrames on the join key into `spark.sql.shuffle.partitions` partitions. For example, `df1.join(df2, "id")` will hash-partition both `df1` and `df2` by the “id” column into 200 partitions (unless configured otherwise) before performing the join. This is essentially like a distributed shuffle where each side is shuffled into the same number of buckets so that matching keys end up in the same partition to be joined. If one DataFrame was originally 100 partitions and the other 50, after the join shuffle both will become 200 partitions (by default).  
  - **Broadcast join optimization:** If one of the DataFrames is small enough (or explicitly flagged to broadcast), Spark will avoid shuffling the small side. In a broadcast join, the larger DataFrame retains its partitions, and the small DataFrame is broadcast to all executors. In this case, the number of tasks for the join equals the number of partitions of the large side (since only that side is being processed in parallel). For instance, if `df1` has 100 partitions and `df2` is tiny and broadcastable, the join will execute with 100 tasks (no shuffle on `df1`, just a broadcast to each task). So `spark.sql.shuffle.partitions` isn’t used here because a shuffle is skipped. Generally, though, for two large tables, Spark will use a shuffle join with the configured number of shuffle partitions.

- **DataFrame `distinct()` or `dropDuplicates()`**: These operations require comparing data across partitions to identify duplicates, which triggers a shuffle as well. Under the hood, distinct is like grouping by all columns to drop dupes. Thus, it will also use `spark.sql.shuffle.partitions` as the number of partitions for the post-shuffle data. E.g. `df.distinct()` on a DataFrame will result in 200 partitions by default after the shuffle (and Spark will usually perform a map-side reduction of duplicates then a shuffle).

- **RDD `reduceByKey` / `groupByKey`:** These are the RDD equivalents of a grouped aggregation. If you do `pairsRDD.reduceByKey(func)` without specifying a partition number, Spark will use the default parallelism logic: it will typically keep the number of partitions equal to the largest upstream RDD partition count, or fallback to `spark.default.parallelism` if it can’t infer it ([Configuration - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/configuration.html#:~:text=,depends%20on%20the%20cluster%20manager)). For example, if `pairsRDD` has 50 partitions and you call reduceByKey with no `numPartitions`, the result RDD will also have 50 partitions (Spark chooses the “largest parent” count). If the RDD had, say, 1 partition, Spark would elevate it to e.g. 8 (default parallelism based on cores) to avoid a single reducer bottleneck ([Differences between spark.sql.shuffle.partitions and spark.default.parallelism](https://kontext.tech/article/1149/differences-between-sparksqlshufflepartitions-and-sparkdefaultparallelism#:~:text=,or%202%2C%20whichever%20is%20larger)). So RDD APIs have a built-in heuristic to propagate or enlarge partition counts for wide ops. This is a key difference: DataFrame API uses a fixed default number (unless overridden), whereas RDD API often uses upstream partition counts dynamically. 

- **RDD `join` (pair RDD):** Similar to reduceByKey, an RDD join will, if not told otherwise, use the max of the two RDDs’ partition counts as the number of shuffle partitions for the join ([Differences between spark.sql.shuffle.partitions and spark.default.parallelism](https://kontext.tech/article/1149/differences-between-sparksqlshufflepartitions-and-sparkdefaultparallelism#:~:text=,or%202%2C%20whichever%20is%20larger)). So if you join an RDD with 60 partitions with another of 40 partitions, the joined RDD will likely have 60 partitions (the larger of the two). If both have 1 partition (and thus that’s below default parallelism on a cluster), it may use the default parallelism value instead to avoid too low parallelism.

- **Repartition and Coalesce:** 
  - Calling `df.repartition(n)` will explicitly shuffle the DataFrame to `n` partitions (overriding the default). If you call `repartition()` and don’t specify `n` but provide a column, it will shuffle by that column using the default shuffle partition count. (For example, `df.repartition(col("key"))` will produce `spark.sql.shuffle.partitions` partitions by default ([Differences between spark.sql.shuffle.partitions and spark.default.parallelism](https://kontext.tech/article/1149/differences-between-sparksqlshufflepartitions-and-sparkdefaultparallelism#:~:text=About%C2%A0spark)).) 
  - `df.coalesce(n)` does **not** do a shuffle; it simply reduces the number of partitions by combining existing partitions into fewer, which is a narrow transformation. Coalesce can only *decrease* partitions (and tries to avoid data movement). It’s often used after a heavy shuffle if you ended up with too many small partitions and want to compact them without another full shuffle.

- **Spark SQL Queries:** If you use Spark SQL (e.g., via `spark.sql("SELECT ...")` or create temp views and run SQL), the behavior is the same as the DataFrame API. A `GROUP BY` or `JOIN` in SQL will use `spark.sql.shuffle.partitions` for the shuffle stage partition count. For instance, running a SQL query that groups or joins large tables will result in 200 shuffle tasks by default. You would need to `SET spark.sql.shuffle.partitions=<value>` in SQL or use the DataFrame API `spark.conf.set` to change that for the query.

In summary, **wide transformations** (operations that require data exchange across partitions, like joins, aggregations, repartitions) will produce new RDDs/DataFrames with partition counts dictated by either `spark.default.parallelism` (for raw RDD operations) or `spark.sql.shuffle.partitions` (for DataFrame/SparkSQL operations). Narrow transformations (maps, filters, etc.) *keep the same number of partitions* as the parent RDD/DataFrame. This is why you might start with a certain partition count, but after a `join` or `groupBy` you see a sudden change to 200 (or some configured number) tasks in the Spark UI – that’s the effect of these configs.

To put it plainly: shuffles happen in operations like **groupBy, reduceByKey, join**, etc., which redistribute data across the cluster. The number of partitions after such a shuffle is controlled by the settings above, and tuning them properly can significantly affect performance.

## 4. Stage Transitions and Partition Counts
Spark executes jobs in a sequence of **stages** separated by shuffle boundaries. Within one stage, tasks correspond to partitions of the data and can execute in parallel. When a shuffle occurs, it marks the end of one stage and the beginning of another, possibly with a different number of partitions (and tasks).

**Narrow vs Wide transformation and stages:** Spark bundles as many narrow transformations (ones that don’t require data shuffle) as possible into the same stage. When a wide transformation (shuffle) is needed, it will break and start a new stage ([How to Optimize Your Apache Spark Application with Partitions - Salesforce Engineering Blog](https://engineering.salesforce.com/how-to-optimize-your-apache-spark-application-with-partitions-257f2c1bb414/#:~:text=Spark%20works%20in%20a%20staged,the%20same%20level%20of%20parallelism)) ([How to Optimize Your Apache Spark Application with Partitions - Salesforce Engineering Blog](https://engineering.salesforce.com/how-to-optimize-your-apache-spark-application-with-partitions-257f2c1bb414/#:~:text=Notice%20that%20often%20when%20we,amount%20of%20rows%20while%20shuffling)). All tasks in a stage share the same number of partitions. For example, consider the following code:
```python
df = spark.read.parquet("data.parquet")            # say this gives 50 partitions
filtered = df.filter(df.value > 0)                 # narrow transformation (still 50 partitions)
grouped = filtered.groupBy("category").count()     # wide transformation -> shuffle
result = grouped.filter(grouped["count"] > 10)     # narrow (post-shuffle)
```
Here:
- The file read might produce `df` with, say, 50 partitions (depending on file splits or upstream config).
- The filter on `df` is narrow, so it stays in the same stage and keeps 50 partitions.
- The groupBy triggers a shuffle. Spark will start a new stage to do the aggregation. In that stage, it will repartition data into `spark.sql.shuffle.partitions` (default 200) partitions for the aggregation. So Stage 1 (the shuffle stage) will have 200 tasks, regardless of the 50 earlier.
- The subsequent filter on `grouped` is again narrow and stays in the same Stage 1, still with 200 partitions.

**Switching from RDD’s parallelism to DataFrame shuffle partitions:** If your job mixes RDD and DataFrame APIs, you might first operate with an RDD at some parallelism, then convert to DataFrame and do a shuffle. The moment you cross into a DataFrame shuffle, the partition count can switch to the `spark.sql.shuffle.partitions` value. For example:

```python
sc.setLocalProperty("spark.default.parallelism", "8")
rdd = sc.parallelize(range(1000))          # uses 8 partitions on an 8-core cluster
df = rdd.map(lambda x: (x % 5, x)).toDF(["key","value"])
print(df.rdd.getNumPartitions())           # likely 8 partitions inherited from rdd
joined = df.join(df, on="key")             # self-join triggers shuffle on 'key'
joined.explain()
```

If you run the self-join, the `explain()` output will show two shuffle exchanges each with 200 partitions by default (for the two sides of the join). The job would execute in multiple stages:
- Stage 0: reading the RDD and converting to DataFrame (8 tasks).
- Stage 1: shuffle each side of the join into 200 partitions, then join (200 tasks for the shuffle/join).
- Stage 2: possibly another shuffle if needed for further operations, etc.

So Spark **“switched”** from 8 partitions in the initial stage to 200 in the shuffle stage. This transition is governed by the configs: the initial RDD used `spark.default.parallelism` (8 here) since we didn’t specify partitions for `parallelize`, and the shuffle used `spark.sql.shuffle.partitions`. 

**Another example (Spark SQL):** If you create a DataFrame from a small file (imagine it has only 1 partition because the file is small) and do a large aggregation:
```python
spark.conf.set("spark.sql.shuffle.partitions", 100)
df = spark.read.json("small.json")  # small file, e.g., results in 1 partition
df.groupBy("category").count().show()
```
Even though `df` had 1 partition initially, the `groupBy` will produce 100 partitions in the shuffle stage (as we set above). Stage 0 will have 1 task (reading the file), Stage 1 will have 100 tasks (the shuffle to do the aggregation). If we hadn’t set it to 100, it would default to 200 tasks for that shuffle, which might be overkill for a small dataset.

The main point is that **each shuffle stage’s parallelism is determined independently**. Spark does not propagate the previous stage’s partition count into a new shuffle stage (unless you explicitly use the RDD API with that behavior). Instead, it uses the configured defaults for that domain (SQL or core). This is why adjusting `spark.sql.shuffle.partitions` is crucial – it directly controls the number of tasks in shuffle stages for DataFrame/SparkSQL jobs. Meanwhile, `spark.default.parallelism` influences how many partitions you get in RDD stages or when reading data without an explicit partition count.

To see these effects in practice, you can check the Spark UI or logs: each stage will list the number of tasks. For DataFrame jobs, you’ll often see stages with 200 tasks (or whatever you set), corresponding to joins/aggregations. If you notice a stage with an odd number like 137 tasks, that might be from an RDD operation or a custom partitioning you did, since 200 is the round default in SQL. 

One more nuance: When Spark reads from certain file sources, it can also use `spark.default.parallelism` as a hint for minimum partitions. For example, `spark.sql.files.minPartitionNum` defaults to `spark.default.parallelism` ([How to Optimize Your Apache Spark Application with Partitions - Salesforce Engineering Blog](https://engineering.salesforce.com/how-to-optimize-your-apache-spark-application-with-partitions-257f2c1bb414/#:~:text=,our%20cluster%2C%20whichever%20is%20bigger)) ([How to Optimize Your Apache Spark Application with Partitions - Salesforce Engineering Blog](https://engineering.salesforce.com/how-to-optimize-your-apache-spark-application-with-partitions-257f2c1bb414/#:~:text=,When%20this%20value%20is)). This means if a file is too small to create enough splits, Spark might pad it to the default parallelism. However, in modern Spark, file reading is usually governed by split size (`maxPartitionBytes`) rather than default parallelism. Still, be aware that `spark.default.parallelism` can indirectly affect initial partition counts in some cases (especially in older Spark versions or certain data source implementations). 

## 5. Optimization Strategies for Parallelism and Shuffle Partitions
Choosing the right number of partitions is a balancing act. Too few partitions (tasks) and your job may under-utilize the cluster (or suffer from data skew with too much data per task). Too many partitions and you incur overhead from task scheduling and coordination. Here are some best practices for tuning `spark.sql.shuffle.partitions` and `spark.default.parallelism`:

- **Use cluster resources effectively:** A common guideline is to have **2–3 tasks per CPU core** in the cluster ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#level-of-parallelism#:~:text=parent%20RDD%E2%80%99s%20number%20of%20partitions,CPU%20core%20in%20your%20cluster)). For example, if you have a 16-core cluster (total across all executors), you might aim for ~32–48 partitions for big shuffles. Spark’s tuning guide suggests this to keep CPUs busy but not overloaded with excessive context-switching. Always consider the total cores: e.g., 200 partitions on a cluster with only 8 cores means 192 tasks will wait, which might be fine if tasks are short, but if tasks are long-running, many will sit idle in the queue.

- **Consider data size (partition size):** Think in terms of data volume per partition. A typical recommendation is to keep partition sizes in the tens to low hundreds of MB. For shuffle outputs, you might target ~100–256 MB of data per partition as a rough heuristic. This is not a hard rule, but for example, if you expect to shuffle 100 GB of data, 200 partitions = 0.5 GB per partition, which might be a bit high (maybe increase partitions), whereas 200 partitions for 1 GB of data = ~5 MB per partition, which is very small (maybe decrease partitions to avoid too many tiny tasks). Some practitioners use formulas like *one partition per 128 MB of input* or *256 MB per partition* ([performance - What is the difference between spark.sql.shuffle.partitions and spark.default.parallelism? - Stack Overflow](https://stackoverflow.com/questions/45704156/what-is-the-difference-between-spark-sql-shuffle-partitions-and-spark-default-pa#:~:text=The%20default%20number%20of%20partition,that%20means%20256MB%20per%20partition)), but the optimal size can vary. Monitor your tasks: if each task is processing gigabytes and running very long, you may need more partitions; if each task is only a few KB, you have too many partitions.

- **Tune `spark.sql.shuffle.partitions` for your job:** The default of 200 is a one-size-fits-all default. For small datasets, 200 tasks can be overkill and add unnecessary overhead. For very large datasets or big clusters, 200 might be too low. For example:
  - If you’re aggregating 1 TB of data on a 100-core cluster, 200 partitions means each task handles ~5 GB – likely too slow and prone to memory issues. Increasing to 1000 partitions (10 tasks per core) might improve throughput.
  - If you’re processing 500 MB of data on a 4-core machine, 200 partitions means ~2.5 MB per task, which might spend more time on scheduling than computation. Reducing to, say, 16 or 32 partitions would be more efficient.
  - You can set this config at the SparkSession or job level. For ad-hoc tuning, calling `spark.conf.set("spark.sql.shuffle.partitions", N)` in PySpark or using `--conf spark.sql.shuffle.partitions=N` when submitting a job can adjust it. You can also tweak it just before a specific operation and set it back, if needed.

- **Tune `spark.default.parallelism` (if using RDDs or reading from certain sources):** If your application uses RDD transformations or you do a lot of `spark.sparkContext.parallelize` or `textFile` reads, ensure `spark.default.parallelism` is set reasonably. On YARN or Kubernetes clusters, Spark will often set it to the total number of cores in your executors, which is usually fine. But if you use dynamic allocation or an environment where total cores isn’t fixed upfront, or if running in local mode, check that it’s not too low. For example, on a large cluster, leaving it at the minimum of 2 could severely under-partition RDD operations. Set it to at least the number of cores or a multiple thereof. You can set it similarly via `spark.conf.set("spark.default.parallelism", X)`. This ensures operations like `reduceByKey` without an explicit partition count don’t end up with a single reducer by accident.  
  - If you’re primarily using DataFrames and Spark SQL, you might not need to touch `spark.default.parallelism` much, but it doesn’t hurt to have it roughly aligned with your cluster size in case it’s needed. It may also serve as a safety net for file sources as mentioned (min partitions when reading).

- **Use `.repartition()` or `.coalesce()` when appropriate:** You have manual control over partitions too. If you know the default behavior isn’t optimal, you can repartition the DataFrame/RDD in your code:
  - After a source read: e.g., `df = spark.read.json(...).repartition(100)` if you know 200 is too high and 100 is enough.
  - Before a join: If one table is huge and another is tiny, you might force the tiny one to not shuffle by broadcasting it (`broadcast(df2)` in code or SQL hint) or ensure they have the same partitioner. If both are huge but one has an existing partitioning that’s useful, you could repartition one DataFrame to match the other’s partition count or key to possibly avoid an extra shuffle (advanced).
  - After a heavy filter: If you filter a DataFrame down to a much smaller size, you might end up with many partitions but each containing little data. Consider coalescing to fewer partitions to reduce overhead in subsequent stages.
  - However, avoid gratuitous repartitioning. Each repartition is a shuffle itself (except coalesce). Only do it when you have a clear reason (e.g., the default 200 partitions is clearly not suitable).

- **Monitor and adjust:** There isn’t a universal “right” number for partitions; it depends on data and cluster. Monitor your Spark UI or logs. If you see tasks taking a very long time and consuming a lot of data, consider increasing partitions. If you see many tasks finishing in under a second and a lot of tasks overall, consider reducing partitions. Also watch out for shuffle file sizes – if they’re excessively large or small, that’s a hint.

- **Leverage Adaptive Query Execution (AQE) in Spark 3+:** Modern Spark (3.x) has adaptive execution which can automatically optimize shuffle partitions at runtime. By default, `spark.sql.adaptive.enabled` is true in Spark 3, and it will coalesce shuffle partitions based on actual data volume. For example, if your shuffle ends up with many partitions that are mostly empty, AQE can cut down the number of partitions dynamically to avoid tiny tasks. It uses settings like `spark.sql.adaptive.advisoryPartitionSizeInBytes` (default 64MB) to aim for partition sizes, and can coalesce partitions after a shuffle stage. Essentially, you might keep `spark.sql.shuffle.partitions` high for parallelism, and let AQE reduce it if the data volume doesn’t require that many. If AQE is on, the initial shuffle still uses `spark.sql.shuffle.partitions` tasks, but they may merge at runtime. If you turn AQE off, then the shuffle partition count is fixed upfront ([How to Optimize Your Apache Spark Application with Partitions - Salesforce Engineering Blog](https://engineering.salesforce.com/how-to-optimize-your-apache-spark-application-with-partitions-257f2c1bb414/#:~:text=,and%20only)) ([How to Optimize Your Apache Spark Application with Partitions - Salesforce Engineering Blog](https://engineering.salesforce.com/how-to-optimize-your-apache-spark-application-with-partitions-257f2c1bb414/#:~:text=If%20we%20set%C2%A0,spark.sql.shuffle.partitions)).  
  - Best practice: keep AQE on (it’s on by default in Spark 3) unless you have a specific reason not to. It can take away some of the guesswork in setting the perfect number of partitions. Still, AQE can’t increase partitions beyond the initial count, it can only reduce (in current implementations), so you should not set the initial shuffle partitions too low. It’s often recommended to set `spark.sql.shuffle.partitions` on the higher end of what you might need, and let AQE coalesce if needed.

- **Example – tuning scenario:** Suppose you have a job that joins two large tables and then does a groupBy. Initially, you used defaults and noticed the join and aggregation each used 200 tasks, and some tasks took a long time (processing hundreds of MB each). Your cluster has 50 cores. You might decide to up `spark.sql.shuffle.partitions` to 500, so each stage has 500 tasks (10 per core). After this change, each task handles less data (maybe ~1/5 the previous amount), possibly improving speed – but at the cost of more overhead. You observe the job again: if tasks are now reasonably sized (say each handles ~100 MB and takes a few seconds), that’s good. If they’re too small (lots of tasks finishing in under a second), you might dial it down to, say, 300. This iterative tuning, guided by actual job statistics, helps find a sweet spot.

In conclusion, **`spark.default.parallelism` and `spark.sql.shuffle.partitions` control how Spark parallelizes tasks at different stages of execution**. DataFrames internally use RDDs, but Spark SQL has its own default for shuffle partitions. Understanding when each config applies (RDD operations vs DataFrame shuffles) is crucial. For pure SQL/DataFrame jobs, focus on `spark.sql.shuffle.partitions` (and consider AQE). For low-level RDD jobs, ensure `spark.default.parallelism` is tuned. And for any Spark job, remember to examine the stage-wise parallelism – an inefficient partitioning can bottleneck an otherwise good algorithm. By applying these concepts and tuning based on your data and cluster, you can achieve much better performance and resource utilization in Spark.

**Sources:**

- Spark documentation – *default parallelism:* Spark decides the default shuffle partitions from the largest upstream RDD or cluster cores ([Configuration - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/configuration.html#:~:text=,depends%20on%20the%20cluster%20manager)). *Shuffle partitions:* default 200 for joins/aggregations in DataFrame ([Configuration - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/configuration.html#:~:text=The%20default%20number%20of%20partitions,from%20the%20same%20checkpoint%20location)).  
- Stack Overflow – clarification that `spark.default.parallelism` applies to RDDs and is ignored for DataFrame shuffles ([performance - What is the difference between spark.sql.shuffle.partitions and spark.default.parallelism? - Stack Overflow](https://stackoverflow.com/questions/45704156/what-is-the-difference-between-spark-sql-shuffle-partitions-and-spark-default-pa#:~:text=,ignored%20when%20working%20with%20dataframes)).  
- Spark official tuning guide – recommends 2–3 tasks per CPU core for optimal parallelism ([Tuning - Spark 3.5.5 Documentation](https://spark.apache.org/docs/latest/tuning.html#level-of-parallelism#:~:text=parent%20RDD%E2%80%99s%20number%20of%20partitions,CPU%20core%20in%20your%20cluster)).  
- Salesforce Engineering Blog – explanation of Spark’s stage and shuffle behavior ([How to Optimize Your Apache Spark Application with Partitions - Salesforce Engineering Blog](https://engineering.salesforce.com/how-to-optimize-your-apache-spark-application-with-partitions-257f2c1bb414/#:~:text=Spark%20works%20in%20a%20staged,the%20same%20level%20of%20parallelism)) ([How to Optimize Your Apache Spark Application with Partitions - Salesforce Engineering Blog](https://engineering.salesforce.com/how-to-optimize-your-apache-spark-application-with-partitions-257f2c1bb414/#:~:text=Notice%20that%20often%20when%20we,amount%20of%20rows%20while%20shuffling)).  
- Kontext Tech – notes on default parallelism vs shuffle partitions differences ([Differences between spark.sql.shuffle.partitions and spark.default.parallelism](https://kontext.tech/article/1149/differences-between-sparksqlshufflepartitions-and-sparkdefaultparallelism#:~:text=,or%202%2C%20whichever%20is%20larger)) ([Differences between spark.sql.shuffle.partitions and spark.default.parallelism](https://kontext.tech/article/1149/differences-between-sparksqlshufflepartitions-and-sparkdefaultparallelism#:~:text=About%C2%A0spark)).  
- Jacek Laskowski’s *Internals of Spark SQL* – DataFrame execution translates to an RDD of internal rows, with `toRdd` as the boundary between Spark SQL and core execution ([QueryExecution - The Internals of Spark SQL](https://books.japila.pl/spark-sql-internals/QueryExecution/#:~:text=The%20%60RDD%60%20is%20the%20top,that%20represent%20physical%20operators)).  
- SparkCodehub – guidance on shuffles in operations like groupBy, reduceByKey, join.